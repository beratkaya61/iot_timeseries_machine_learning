{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "MODEL_PATH = \"../models/llm_forecaster/\"\n",
    "VAL_FILE = \"../data/llm_preprocessed/val.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH)\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_PATH)\n",
    "model.eval()\n",
    "\n",
    "#look difference between this\n",
    "#tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "#model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR).to(device)\n",
    "\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(VAL_FILE)\n",
    "\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    prompt = row['prompt']\n",
    "\n",
    "    # Encode prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, max_length=inputs.input_ids.shape[1] + 10, num_beams=3, early_stopping=True)\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # Extract the predicted number from the generated text\n",
    "    try:\n",
    "        predicted_numbers = eval(generated_text.split(\":\")[-1].strip())\n",
    "        if isinstance(predicted_numbers, list):\n",
    "            prediction = predicted_numbers[0]  # Only first number if list\n",
    "        else:\n",
    "            prediction = predicted_numbers\n",
    "    except:\n",
    "        prediction = np.nan\n",
    "\n",
    "    try:\n",
    "        true_numbers = eval(row['completion'])\n",
    "        if isinstance(true_numbers, list):\n",
    "            true_value = true_numbers[0]\n",
    "        else:\n",
    "            true_value = true_numbers\n",
    "    except:\n",
    "        true_value = np.nan\n",
    "\n",
    "    predictions.append(prediction)\n",
    "    ground_truths.append(true_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2. Metrics\n",
    "predictions = np.array(predictions)\n",
    "ground_truths = np.array(ground_truths)\n",
    "\n",
    "mse = mean_squared_error(ground_truths, predictions)\n",
    "mae = mean_absolute_error(ground_truths, predictions)\n",
    "smape = np.mean(2.0 * np.abs(predictions - ground_truths) / (np.abs(predictions) + np.abs(ground_truths))) * 100\n",
    "\n",
    "print(f\"âœ… Evaluation done!\")\n",
    "print(f\"ðŸ”µ MSE: {mse:.4f}\")\n",
    "print(f\"ðŸŸ¢ MAE: {mae:.4f}\")\n",
    "print(f\"ðŸŸ  SMAPE: {smape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
